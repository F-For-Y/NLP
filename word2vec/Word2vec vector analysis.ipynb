{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Word2vec Vector Analysis\n",
    "\n",
    "*Important Note:* Start this notebook only after you've gotten your word2vec model up and running!\n",
    "\n",
    "Many NLP packages support working with word embeddings. In this notebook you can work through the various problems assigned in Task 3. We've provided the basic functionality for loading word vectors using [Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html), a good library for learning and using word vectors, and for working with the vectors. \n",
    "\n",
    "One of the fun parts of word vectors is getting a sense of what they learned. Feel free to explore the vectors here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('./Save_Vector/word2vec_batch256_med.kv', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19817095,  0.01267677, -0.07539976,  0.03114463,  0.00072547,\n",
       "       -0.07067546, -0.01884538,  0.31666568,  0.04441987,  0.1983985 ,\n",
       "        0.16294172, -0.05090679,  0.12091672, -0.0126444 , -0.22901875,\n",
       "        0.05950258,  0.22174393,  0.20474888,  0.01465054, -0.19564652,\n",
       "       -0.29659438,  0.05398606, -0.17579515, -0.11693859,  0.13066602,\n",
       "       -0.04674801,  0.06568433,  0.06738054,  0.1682875 ,  0.11984465,\n",
       "       -0.24535231,  0.18994574, -0.14271109, -0.10922622,  0.03835478,\n",
       "        0.05750776, -0.00797755,  0.13742296,  0.0146268 , -0.01521034,\n",
       "       -0.01826614,  0.08698475,  0.04549011,  0.22656283,  0.06423106,\n",
       "       -0.1256823 , -0.10989477, -0.05961025, -0.06788946,  0.04902486],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(a, b, c):\n",
    "    return word_vectors.most_similar(positive=[b, c], negative=[a])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stephen'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 15\n",
    "- 10 Target Word and Compute the Most Similar for Each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "commom_words = ['books', 'love', 'awesome', 'excellent']\n",
    "occasion_words = ['kill', 'rock', 'president', 'sequel']\n",
    "rare_words = ['honorable', 'patriot', 'honeymoon', 'january']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books  -----  [('novels', 0.8390048742294312), ('previous', 0.7793611288070679)]\n",
      "love  -----  [('loved', 0.7388542890548706), ('triangle', 0.6549731492996216)]\n",
      "awesome  -----  [('amazing', 0.913963794708252), ('fantastic', 0.8079924583435059)]\n",
      "excellent  -----  [('outstanding', 0.8173568248748779), ('awesome', 0.7800126075744629)]\n"
     ]
    }
   ],
   "source": [
    "for word in commom_words:\n",
    "    print(word, ' ----- ', word_vectors.similar_by_word(word)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill  -----  [('revert', 0.8278816342353821), ('marry', 0.7633633613586426)]\n",
      "rock  -----  [('briggs', 0.8888666033744812), ('steinbeck', 0.8461424112319946)]\n",
      "president  -----  [('whines', 0.9308725595474243), ('considers', 0.8895583152770996)]\n",
      "sequel  -----  [('next', 0.6991185545921326), ('installment', 0.6888800859451294)]\n"
     ]
    }
   ],
   "source": [
    "for word in occasion_words:\n",
    "    print(word, ' ----- ', word_vectors.similar_by_word(word)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honorable  -----  [('union', 0.9074601531028748), ('hub', 0.8979661464691162)]\n",
      "patriot  -----  [('lucien', 0.9164902567863464), ('servants', 0.9090864658355713)]\n",
      "honeymoon  -----  [('identity', 0.8583592176437378), ('heavenly', 0.8547177910804749)]\n",
      "january  -----  [('november', 0.9296455979347229), ('september', 0.9283466935157776)]\n"
     ]
    }
   ],
   "source": [
    "for word in rare_words:\n",
    "    print(word, ' ----- ', word_vectors.similar_by_word(word)[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for commom wors, the most similar words (2000-10000) are really relevant. For occasion words (100-1000), sequle and next, president and reagan are relevant while the other two are not. For rare words, only january are relevant to july while the other three don't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dollars'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('america', 'dollar', 'europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fabulous'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('cold', 'winter', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'double'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('caffeine', 'coffee', 'alcohol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fatherhood'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('day', 'sun', 'night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reviews'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('papers', 'books', 'digital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('pdf', 0.9319278001785278),\n",
       "  ('electronic', 0.931271493434906),\n",
       "  ('card', 0.8917281031608582),\n",
       "  ('app', 0.8829511404037476),\n",
       "  ('software', 0.8798503875732422),\n",
       "  ('gluten', 0.8782529234886169),\n",
       "  ('exam', 0.8695907592773438),\n",
       "  ('dvd', 0.8690330386161804),\n",
       "  ('downloadable', 0.8546414971351624),\n",
       "  ('ios', 0.851502001285553)],\n",
       " [('sexy', 0.9043593406677246),\n",
       "  ('sizzling', 0.8066380023956299),\n",
       "  ('steamy', 0.8016305565834045),\n",
       "  ('creepy', 0.7914137244224548),\n",
       "  ('lust', 0.7869770526885986),\n",
       "  ('romantic', 0.7845473289489746),\n",
       "  ('sweet', 0.7829388380050659),\n",
       "  ('insta', 0.7622817754745483),\n",
       "  ('quirky', 0.7579652667045593),\n",
       "  ('murder', 0.7565175890922546)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('digital'), word_vectors.similar_by_word('hot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
