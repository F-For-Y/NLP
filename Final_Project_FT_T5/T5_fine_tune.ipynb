{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for Topic News Title Summarization \n",
    "\n",
    "- In the era of digital information, the volume of news content available to readers has grown exponentially, making it increasingly challenging for individuals to stay informed without becoming overwhelmed. The project's primary goal is to leverage machine learning (ML) techniques for the effective summarization of news articles, aiming to improve the efficiency, accuracy, and readability of these summaries, allowing readers to grasp the essence of news stories without dedicating extensive time to reading full articles.\n",
    "\n",
    "- The stakeholders of this project can be individual readers, news organizations, educational sectors, and potentially government bodies reliant on swift and accurate information dissemination. Improved news summarization models can transform media consumption by providing accessible, succinct summaries of complex news stories, thereby enhancing public knowledge and engagement. Additionally, in broader vew, enhanced news summarization techniques could pave the way for similar advancements in summarizing other forms of text, such as academic literature, legal documents, and social media feeds.\n",
    "\n",
    "- Potential Model We will Explore:\n",
    "    - Bert summarization\n",
    "    - Fint tune T5-small\n",
    "    - GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune the T5-small model. At the Overview page of the Hugging Face T5 model, it provides the following tips:\n",
    "\n",
    "- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.\n",
    "- T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: …."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer, EarlyStoppingCallback\n",
    "from transformers import EvalPrediction\n",
    "from datasets import load_metric, load_dataset\n",
    "from bert_score import score\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'mps' device\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "device = 'cpu' \n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Tokenizer and Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-small'\n",
    "\n",
    "# TODO: Load the tokenizer using AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=1024)\n",
    "\n",
    "# TODO: Load Pre-trained model from HuggingFace Model Hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 60506624 || all params: 60506624 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "## Let's see how many parameters we are going to be changing\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_set.csv', sep=',')\n",
    "test_df = pd.read_csv('data/test_set.csv', sep=',')\n",
    "dev_df = pd.read_csv('data/dev_set.csv', sep=',')\n",
    "\n",
    "train_df['content'] = train_df['content'].apply(lambda x: re.sub(r'[\\r\\n]+', ' ', x))\n",
    "test_df['content'] = test_df['content'].apply(lambda x: re.sub(r'[\\r\\n]+', ' ', x))\n",
    "dev_df['content'] = dev_df['content'].apply(lambda x: re.sub(r'[\\r\\n]+', ' ', x))\n",
    "\n",
    "labels = ['title']\n",
    "target_col = ['data_id', 'content', 'title']    \n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[target_col])    \n",
    "dev_ds = Dataset.from_pandas(dev_df[target_col])\n",
    "test_ds = Dataset.from_pandas(test_df[target_col].iloc[:1200])  \n",
    "\n",
    "dataset_dict = DatasetDict({    \n",
    "    'train': train_ds,\n",
    "    'dev': dev_ds,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Prepends the string \"summarize: \" to each document in the 'text' field of the input examples.\n",
    "    # This is done to instruct the T5 model on the task it needs to perform, which in this case is summarization.\n",
    "    inputs = [\"Generate title: \" + doc for doc in examples[\"content\"]]\n",
    "\n",
    "    # Tokenizes the prepended input texts to convert them into a format that can be fed into the T5 model.\n",
    "    # Sets a maximum token length of 1024, and truncates any text longer than this limit.\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, return_tensors=\"pt\", padding='longest')\n",
    "\n",
    "    # Tokenizes the 'summary' field of the input examples to prepare the target labels for the summarization task.\n",
    "    # Sets a maximum token length of 128, and truncates any text longer than this limit.\n",
    "    labels = tokenizer(text_target=examples[\"title\"], max_length=32, truncation=True, return_tensors=\"pt\", padding='longest')\n",
    "\n",
    "    # Assigns the tokenized labels to the 'labels' field of model_inputs.\n",
    "    # The 'labels' field is used during training to calculate the loss and guide model learning.\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Returns the prepared inputs and labels as a single dictionary, ready for training.\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7c7637cfd349f4bf1d18d46cb37a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d195b74313eb4b61a4d644d7ec510594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6960ffbdac35499e84fb5c5f0260d219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['data_id', 'content', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8400\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['data_id', 'content', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['data_id', 'content', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate title: (Natural News) Pancreatic cancer is one of the most daunting varieties of the disease, with very few people surviving beyond five years after their diagnosis. With few symptoms, most patients aren’t diagnosed until they’ve already reached the metastatic stage. It’s the fourth leading cause of cancer deaths and the 12th most common type of cancer worldwide, and an effective treatment is desperately needed. Now, a new discovery could bring hope to future patients. Scientists at Tel Aviv University have developed a new treatment that could destroy pancreatic cancer cells. Their treatment involves a small molecule known as PJ34, which was originally developed to help stroke victims. They discovered that when it is injected, it causes human cancer cells to destroy themselves during cell division, or mitosis. They conducted their study using transplanted human pancreatic cancer in immunocompromised mice. They found that after two weeks of daily injection with the molecule, there was an incredible 90 percent reduction in cancer cells 30 days after ending the treatment, with one mouse even seeing its tumor disappear completely. The researchers say this molecule spurs an anomaly while the cancer cells are dividing that provokes rapid cell death. In other words, it’s the cell multiplication itself that is causi ng these bad cells to die. Best of all, unlike traditional treatments like chemotherapy, PJ34 does not affect healthy cells, so they did not see any negative effects in the mice from treatment. In fact, the animals grew and gained weight as usual. Sponsored: NEW Biostructured Silver First Aid Gel created by the Health Ranger combines three types of silver (ionic silver, colloidal silver, biostructured silver) with seven potent botanicals (rosemary, oregano, cinnamon and more) to create a breakthrough first aid silver gel. Over 50 ppm silver, verified via ICP-MS lab analysis. Made from 100% Texas rain water and 70% solar power. Zero chemical preservatives, fragrances or emulsifiers. See full details here. The study’s leader, Professor Malka Cohen-Armon, also explored this mechanism in a 2017 study, when she used it to treat triple-negative breast cancer, which, like pancreatic cancer, is notoriously difficult to treat. The research team has also found in parallel studies that the molecule is efficient on other deadly types of cancer, including aggressive varieties of lung, brain, ovarian and breast cancers that resist current therapies. A statement released by the university said the research holds “great potential for the development of a new effective therapy to treat this aggressive cancer in humans.” Next, the researchers would like to test the molecule on bigger animals like pigs before eventually carrying out trials in humans, which they believe could take around two years, depending on the funding available. The findings were published in the October edition of the peer-reviewed biomedical journal Oncotarget. Last year, scientists discovered another potential therapy for pancreatic cancer in the form of dual thermal ablation, a process that entails heating and freezing. They used different technologies to heat and freeze cancer cells and measure the levels of cell death and regrowth. When heating or freezing are used individually, some cells may die but others will survive and regrow. With dual-thermal ablation, however, more cells die and don’t return. Although these treatments are very promising, it’s still a good idea to do everything you can to reduce your risk, especially if someone in your family has had this type of cancer. Following a healthy diet and maintaining a healthy wright are two easy ways to lower your risk, and you should also refrain from smoking. The Israeli study, and others like it, could lead to incredibly valuable treatments for this deadly disease, and it’s encouraging to see researchers thinking outside of the box and looking to develop treatments that don’t put your overall health at risk the way chemotherapy does.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Israeli scientists find a promising potential treatment for pancreatic cancer</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_datasets['train'][0]\n",
    "\n",
    "# We can use the tokenizer to reverse the ID mapping to see what the text was\n",
    "print(tokenizer.decode(example['input_ids']))\n",
    "print(tokenizer.decode(example['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set up the model and training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    seed = 12345, \n",
    "    do_eval=True,\n",
    "    output_dir=\"my_fine_tuned_t5_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    # weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=210,\n",
    "    save_steps=420,\n",
    "    evaluation_strategy=\"steps\",       \n",
    "    save_strategy=\"steps\",  \n",
    "    load_best_model_at_end=True, \n",
    "    predict_with_generate=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    "    # fp16=True,\n",
    "    report_to='wandb',\n",
    "    logging_dir='./t5_logs',\n",
    "    run_name=\"t5-base-title-summarizer\",  \n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    # Unpacks the evaluation predictions tuple into predictions and labels.\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decodes the tokenized predictions back to text, skipping any special tokens (e.g., padding tokens).\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replaces any -100 values in labels with the tokenizer's pad_token_id.\n",
    "    # This is done because -100 is often used to ignore certain tokens when calculating the loss during training.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decodes the tokenized labels back to text, skipping any special tokens (e.g., padding tokens).\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Computes the ROUGE metric between the decoded predictions and decoded labels.\n",
    "    # The use_stemmer parameter enables stemming, which reduces words to their root form before comparison.\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # Calculates the length of each prediction by counting the non-padding tokens.\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "\n",
    "    # Computes the mean length of the predictions and adds it to the result dictionary under the key \"gen_len\".\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Rounds each value in the result dictionary to 4 decimal places for cleaner output, and returns the result.\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fz/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t5_trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate the Predicted Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = t5_trainer.predict(tokenized_datasets['test']) \n",
    "print(test_title)\n",
    "result = []\n",
    "for title_ids in test_title.predictions:\n",
    "    result.append(tokenizer.decode(title_ids, skip_special_tokens=True))\n",
    "print(result[0])\n",
    "test_set = test_df[['data_id', 'title']].iloc[:1200].copy()\n",
    "test_set['predicted_title'] = result\n",
    "test_set.to_csv('data/test_set_summary_t5_large.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succcessfully load the test_set_summary_t5_large.csv\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/test_set_summary_t5_large.csv'):\n",
    "    print('succcessfully load the test_set_summary_t5_large.csv')\n",
    "    test_set_t5 = pd.read_csv('data/test_set_summary_t5_large.csv')\n",
    "else:\n",
    "    print('failed to load the test_set_summary_t5_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.3812477152513559,\n",
       " 'rouge2': 0.1745242266745301,\n",
       " 'rougeL': 0.33269447741054975,\n",
       " 'rougeLsum': 0.3324886814077494}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=test_set_t5['predicted_title'], references=test_set_t5['title'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0449ae1abc8c477f8f9b3ed36f021aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028371ae9a8e41959adef0903b97b0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 56.55 seconds, 21.22 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score(test_set_t5['predicted_title'].to_list(), test_set_t5['title'].to_list(), lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8933\n",
      "Recall: 0.8835\n",
      "F1: 0.8882\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {P.mean():.4f}')\n",
    "print(f'Recall: {R.mean():.4f}')\n",
    "print(f'F1: {F1.mean():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
