{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for Topic News Title Summarization \n",
    "\n",
    "- In the era of digital information, the volume of news content available to readers has grown exponentially, making it increasingly challenging for individuals to stay informed without becoming overwhelmed. The project's primary goal is to leverage machine learning (ML) techniques for the effective summarization of news articles, aiming to improve the efficiency, accuracy, and readability of these summaries, allowing readers to grasp the essence of news stories without dedicating extensive time to reading full articles.\n",
    "\n",
    "- The stakeholders of this project can be individual readers, news organizations, educational sectors, and potentially government bodies reliant on swift and accurate information dissemination. Improved news summarization models can transform media consumption by providing accessible, succinct summaries of complex news stories, thereby enhancing public knowledge and engagement. Additionally, in broader vew, enhanced news summarization techniques could pave the way for similar advancements in summarizing other forms of text, such as academic literature, legal documents, and social media feeds.\n",
    "\n",
    "- Potential Model We will Explore:\n",
    "    - Bert summarization\n",
    "    - Fint tune T5-small\n",
    "    - Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from rouge import Rouge\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extractive Bert Summarization without Fine Tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer, TransformerSummarizer    \n",
    "\n",
    "# Initialize the BERT model\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('data/test_set.csv')\n",
    "test_set['content'] = test_set['content'].apply(lambda x: re.sub(r'[\\r\\n]+', ' ', x))\n",
    "test = test_set.head().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test_set_summary_bert.csv already exists, skipping the generation of summaries\n"
     ]
    }
   ],
   "source": [
    "# Use the model to generate summaries\n",
    "def generate_summary(texts, model):\n",
    "    summaries = []\n",
    "    length = len(texts)\n",
    "    for i in tqdm(range(length)):\n",
    "        context = texts[i]\n",
    "        summ = model(context, num_sentences=1)  \n",
    "        summaries.append(summ)\n",
    "    return summaries\n",
    "\n",
    "# 判断 test_set_summary_bert.csv 是否存在，如果存在则不运行以下代码：\n",
    "if os.path.exists('data/test_set_summary_bert.csv'):\n",
    "    print('data/test_set_summary_bert.csv already exists, skipping the generation of summaries')\n",
    "else:\n",
    "# 假设 test['content'] 是一个包含文本的列表\n",
    "    summaries = generate_summary(test_set['content'].tolist(), model)\n",
    "    test_set['summary_bert'] = summaries\n",
    "    test_set = test_set[['data_id', 'title', 'content_length', 'category_level_1', 'category_level_2', 'summary_bert']]\n",
    "# save the result:\n",
    "    test_set.to_csv('data/test_set_summary_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Calculate the Rouge Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/test_set_summary_bert.csv'\n",
    "metric_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ROUGE score based on the column of 'summary_bert' and 'title'\n",
    "def calculate_average_rouge(df, summary_col='summary_bert', reference_col='title'):\n",
    "    rouge = Rouge()\n",
    "    scores = {'rouge-1': {'f': []}, 'rouge-2': {'f': []}, 'rouge-l': {'f': []}}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        score = rouge.get_scores(row[summary_col], row[reference_col])[0]\n",
    "        for key in score:\n",
    "            scores[key]['f'].append(score[key]['f'])\n",
    "    \n",
    "    # Calculating the average scores\n",
    "    avg_scores = {key: {'f': sum(values['f']) / len(values['f'])} for key, values in scores.items()}\n",
    "    \n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE scores: {'rouge-1': {'f': 0.20443960614422044}, 'rouge-2': {'f': 0.07319228343811812}, 'rouge-l': {'f': 0.17942339480933162}}\n"
     ]
    }
   ],
   "source": [
    "avg_rouge_scores = calculate_average_rouge(metric_df)\n",
    "print(\"Average ROUGE scores:\", avg_rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Calculate the Bert-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63056e8a435742f58db9f75f4de953b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb9cfe55bd4586b0895933df9054c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2722f1bf01a249a1bc7ab55f32e54bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 80.73 seconds, 15.08 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "# Calculate BERTScore\n",
    "P, R, F1 = score(metric_df['summary_bert'].to_list(), metric_df['title'].to_list(), lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8582, Recall: 0.8844, F1 score: 0.8708\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1 score: {F1.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
